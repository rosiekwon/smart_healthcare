{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5eWp80XgW6C/ZbU+k38Yl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosiekwon/smart_healthcare/blob/main/WEEK%203/Deep_Learning_concept.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear classifier"
      ],
      "metadata": {
        "id": "0091SxQK4SMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis\n",
        "\n",
        "input data를 받아서 정답을 예측해주는 모델(함수)\n",
        "\n",
        "weight * input + bias = scores\n",
        "\n",
        "-> decision boundary [dimension column vector(weight)과 scalar value(bias)에 의해 결정]"
      ],
      "metadata": {
        "id": "_GkFESaYjuCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function\n",
        "\n",
        "hypothesis의 성능을 확인하기 위한 지표\n",
        "* 0-1 loss\n",
        "* asymmetrix loss"
      ],
      "metadata": {
        "id": "z7anNd6HnC6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression\n",
        "\n",
        "linear classifier + uncertainty\n",
        "- Negative log likelihood(NLL) loss"
      ],
      "metadata": {
        "id": "t6vMxUKv3nP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed forward Neural network\n",
        "\n",
        "복잡한 데이터를 잘 분류하기 위해 input으로 더 많은 feature을 넣어서 복잡한 classifier를 만들어준다\n",
        "\n",
        "<img src='https://static.packt-cdn.com/products/9781788397872/graphics/1ebc2a0a-2123-4351-b7e1-eb57f098bafa.png' height= 300, width=300> "
      ],
      "metadata": {
        "id": "Cihwz5Yh4wp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation function\n",
        "<img src ='https://1394217531-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvNWUoWieQqaGmU_gl9%2F-LvO3qs2RImYjpBE8vln%2Factivation-functions3.jpg?alt=media&token=f96a3007-5888-43c3-a256-2dafadd5df7c' height=400 width =400> "
      ],
      "metadata": {
        "id": "jND1q7ml7Wqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient descent (경사하강법)\n",
        "Loss function을 수행할 때 이 함수를 미분하여 근사값(기울기)를 구하고 경사의 반대방향으로 계속 이동시켜 최소 값을 이를 때까지 반복한 것"
      ],
      "metadata": {
        "id": "ozQmU2VY4zcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 오차역전파\n",
        "\n",
        "학습을 위해 loss를 가지고 역전파 계산과정에 사용하며, 오차를 각 가중치로 미분한다"
      ],
      "metadata": {
        "id": "TyPXylbfFlbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "\n",
        "- 손실함수의 결과값을 최소화하는 모델의 가중치를 찾는 것\n",
        "- gradient descent는 가장 기본적인 optimizer의 하나로 경사를 따라 손실함수의 최소방향을 찾아가면서 가중치 업데이트 \n",
        "\n",
        "- i.e. SGD,Adam,Momentum,AdaGrad,AdaDelta"
      ],
      "metadata": {
        "id": "U-Fp8joOGVQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic Gradient descent(SDG)\n",
        "확률적 경사하강법으로 random하게 추출된 일부 데이터에 대해 gradient를 확인하고 이동\n",
        "- 단점\n",
        "\n",
        "  * 고차원의 hypothesis 안에서는 local minimum 보다 saddle point(안장점)에서 문제가 발생한다\n",
        "  * 탐색 경로가 비효율적이다\n",
        "\n",
        "### Momentum\n",
        "기울기의 방향으로 속도가 생성되며 초기에는 0일 것이고 그 방향으로 gradient이동이 지속되면 더 큰 운동량을 갖게 된다\n",
        "\n",
        "### AdaGrad\n",
        "학습이 진행될 수록 학습률을 줄여주는 방식\n",
        "\n",
        "### RMSprop\n",
        "AdaGrad에서 기울기를 추가로 반영하기 위해 지수 가중 이동평균이 추가된 방법이다\n",
        "\n",
        "### Adam\n",
        "Momemtum과 RMSprop을 함께 적용한 방법이다. momentum의 관성을 이용하기 때문에 local minima나 saddle point에 빠지지 않고 RMSprop을 적용해서 lr(학습률)이 자동적으로 조절된다."
      ],
      "metadata": {
        "id": "LCorWbw7G3Fy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNHLgdNljF93"
      },
      "outputs": [],
      "source": []
    }
  ]
}